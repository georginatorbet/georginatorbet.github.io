---
layout: post
title: "Psychology and the Replication problem"
date: 2015-01-10
---
Academic psychology has been forced to do some self-examination of late, not only in the wake of high-profile cases of scientific fraud but also after the widely reported publication of some dubious results. The work of Daryl Bem (2011) which appeared to find parapsychological abilities such as clairvoyance in the general student population has had an impact not only on the academic literature, but also on the way that psychology experiments are published and publicized. When Bem's experiments were repeated by Ritchie, Wiseman, and French, researchers who were dubious about his results, they did not find evidence of the parapsychological abilities that he did. But they struggled to find a journal that was willing to publish their results, as they were a repetition of previously published work.

This throws into the spotlight one of academic science’s biggest problems — the bias inherent in which results are published and which results never see the light of day. Known as the file-drawer problem, this occurs as experiments that yield strong, positive, novel results are more likely to be published than those that yield negative findings. Thus, on reviewing the literature, one sees the results of only a fraction of the experiments that have actually been performed and may be persuaded that a finding is more robust than it actually is.

Ed Yong examines how this issue is particularly relevant to psychology in a recent Nature news feature. He argues that because psychology research is often of interest to the public, findings that are unusual or counterintuitive tend to be widely publicized and discussed. This attitude rubs off on journal editors and reviewers too, pushing to publish only novel or interesting findings. A gradual evolution of ideas through thorough and meticulous experimentation might in theory be how science operates, but in practice the drive for novelty is biasing the accessible summation of available scientific knowledge.

A further problem specific to the field of psychology is that large sample sizes and flexible hypotheses mean that experimental findings can be tweaked in order to make positive results more likely. If, for example, an experiment on memory is carried out on a large group (say N = 300) and does not yield any significant results, researchers may change exclusion criteria to remove outliers or focus on a particular age group or gender. They may focus their reporting on just one particular task that was significant, or just keep collecting more data until they have an N that does produce significant results. The range of possible tweaks almost guarantees that some kind of significant result can be produced from any given experiment.

What can be done to address this serious problem? First, academic journals should move away from strict requirements of novelty for submissions. Admittedly, no one wants to read new studies that merely reproduce well-known and uncontroversial findings. However, many open questions remain regarding new findings and theories, and differing perspectives can be valuable. The journal PLoS One, for example, accepts submissions based on the quality of their experimental methodology rather than on their having positive results. Second, researchers should be encouraged to take an active interest in good scientific practice, including keeping a record of their intended method of statistical analysis before the data is collected. Finally, one must wonder if the focus that is common in universities and funding bodies on publications as indicative of scientific excellence could be re-directed. The publish or perish culture may encourage researchers to value the quantity of their publications over and above their quality and scientific rigour.


Ritchie, S. J., Wiseman, R., & French, C. C. (2012).  Replication, replication, replication. The Psychologist, 25 (5): 346–357. 

Yong, E. (2012).  Replication studies: Bad copy.  Nature, 485 (7398): 298–300.
